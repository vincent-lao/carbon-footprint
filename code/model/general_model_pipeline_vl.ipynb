{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model Data Pipeline\n",
    "Created by Vincent Lao.\n",
    "\n",
    "### Pipeline:\n",
    "clean data $\\rightarrow$ `preprocess_data()` $\\rightarrow$ `run_model()`  \n",
    "$\\hspace{4cm} or$  \n",
    "clean data $\\rightarrow$ `preprocess_data()` $\\rightarrow$ `linreg_kfold_cv()`  \n",
    "$\\hspace{4cm} or$  \n",
    "clean data $\\rightarrow$ `preprocess_data()` $\\rightarrow$ `tree_kfold_cv()`   \n",
    "$\\hspace{4cm} or$  \n",
    "clean data $\\rightarrow$ `preprocess_data()` $\\rightarrow$ `forward_selection()`   \n",
    "\n",
    "### Function of Pipeline:\n",
    "\n",
    "Given a dataset, y variable column name, and a defined model (e.g. LinearRegression()), do the following:  \n",
    "1. Split the data into training and testing data.\n",
    "2. Train the model on the training set, and predict on the test set.\n",
    "3. Calculate the MSE, and plot some diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# !pip install mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, y_col, test_size=0.2, random_state=100, standardize_cols = [None]):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and splits it up into training and testing data, \n",
    "    as well as features and response variable.\n",
    "    \n",
    "    Input: data,             a Pandas dataframe\n",
    "           y_col,            a string that is the name of the response variable\n",
    "           test_size,        a float between 0 and 1 indicating the fraction of the data to include in the test split\n",
    "           random_state,     an integer, used to define the random state\n",
    "           standardize_cols, a list of columns you would like to standardize. by default, standardizes none.\n",
    "           \n",
    "    Output: X_train, 2D array of the training data feature matrix\n",
    "           X_test,  1D array of the training data response variable\n",
    "           y_train, 2D array of the testing data feature matrix\n",
    "           y_test,  1D array of the testing data response variable\n",
    "    \"\"\"\n",
    "    \n",
    "    if any(standardize_cols):\n",
    "        print('Standardizing data...')\n",
    "        data = data.copy().reset_index()\n",
    "        data = data.drop(columns=data.columns[0])\n",
    "        \n",
    "        standardize_features = normalize(data.loc[:, standardize_cols], axis=0)\n",
    "        standardize_df = pd.DataFrame(standardize_features, columns=standardize_cols)\n",
    "        \n",
    "        data.drop(standardize_cols, axis = 1, inplace = True)\n",
    "        data = pd.concat([data, standardize_df], axis= 1)\n",
    "    \n",
    "    print('Splitting data...')\n",
    "    X = data.loc[:, data.columns != y_col]\n",
    "    y = data[y_col]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                                            test_size=test_size, \\\n",
    "                                                            random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(X_train, X_test, y_train, y_test, model, diagnostics = False):\n",
    "    \"\"\"\n",
    "    This function takes in data that has been split by scikit-learn's test_train_split\n",
    "    and a model that has been initialized, and fits the model and calculates the MSE on the training & testing data.\n",
    "    \n",
    "    Input: X_train, X_test, y_train, y_test, the output of preprocess_data()\n",
    "    \n",
    "           model, an initialized scikit-learn model, i.e. LinearRegression, Ridge(), Lasso(), \n",
    "                                                          DecisionTreeRegressor(), RandomForestRegressor\n",
    "           diagnostics, a boolean indicating if you would like to plot linear regression diagnostic plots\n",
    "           \n",
    "    Ouput: printed R^2 and MSE values of both the training and testing sets\n",
    "           model, the fitted model\n",
    "    \"\"\"\n",
    "\n",
    "    # fit model with scikit-learn\n",
    "    print('Fitting Model...')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds_train = model.predict(X_train)\n",
    "    mse_train = mean_squared_error(preds_train, y_train.values)\n",
    "    print('[Train MSE:', str(np.round(mse_train, 4)) + ']')\n",
    "    \n",
    "    r_2 = model.score(X_train, y_train)\n",
    "    print('[Train R^2:', str(r_2) + ']')\n",
    "    \n",
    "    # evaluate model\n",
    "    preds_test = model.predict(X_test)\n",
    "    mse_test = mean_squared_error(preds_test, y_test.values)\n",
    "    print('[Test MSE:', str(np.round(mse_test, 4)) + ']')\n",
    "    \n",
    "    r_2 = model.score(X_test, y_test)\n",
    "    print('[Test R^2:', str(r_2) + ']')\n",
    "    \n",
    "    # plot any diagonostics that are in the \n",
    "    if diagnostics:\n",
    "        summarize_diagnostics(preds_test, y_test)\n",
    "\n",
    "    print('----FINISHED----')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def summarize_diagnostics(preds, y_test):\n",
    "    \n",
    "    df = pd.DataFrame({'preds' : preds, \n",
    "                       'y_test' : y_test, \n",
    "                       'resids' : y_test - preds}).sort_values('preds')\n",
    "\n",
    "    # plot residuals\n",
    "    plt.subplots(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    plt.title('Fitted Values vs. Residuals')\n",
    "    plt.scatter(df['preds'], df['resids'], color='blue')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.title('Fitted Values vs. True Values')\n",
    "    plt.scatter(df['preds'], df['y_test'], color='blue')\n",
    "    plt.ylabel('True Values')\n",
    "    plt.xlabel('Fitted Values')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save plot to file\n",
    "    #  plt.savefig('../../visualizations/diagnostic_plot.png')\n",
    "    #  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_kfold_cv(ModelCV, X_train, X_test, y_train, y_test, alphas, n_splits = 5, random_state=100):\n",
    "    \"\"\"\n",
    "    This function takes either RidgeCV or LassoCV, and performs cross validation on the input data.\n",
    "    Be careful to standardize the input data beforehand!\n",
    "    \n",
    "    Input: ModelCV,      either RidgeCV or LassoCV (before initializing it).\n",
    "           X_train,      X_test, y_train, y_test, output from preprocess_data()\n",
    "           alphas,       a list or array of alpha values you would like to test\n",
    "           n_splits,     number of folds you would like for your cv\n",
    "           random_state, seed of the cross-validation for reproducibility\n",
    "           \n",
    "    Output: print statements of the optimal alpha value + test set MSE\n",
    "            alpha_opt, the chosen optimal alpha value on the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KFold(n_splits = n_splits, shuffle=True, random_state=random_state)\n",
    "    cv = ModelCV(cv = kf, alphas = alphas)\n",
    "    cv.fit(X_train, y_train)\n",
    "    \n",
    "    alpha_opt = cv.alpha_\n",
    "    print(\"optimal alpha:\", alpha_opt)\n",
    "    \n",
    "    y_pred_cv = cv.predict(X_test)\n",
    "    \n",
    "    cv_mse = mean_squared_error(y_test, y_pred_cv)\n",
    "    print(\"Test MSE with cross-validated\", re.findall(r'\\w{5}CV', str(ModelCV))[0] + \":\", cv_mse)\n",
    "    \n",
    "    r_2 = cv.score(X_test, y_test)\n",
    "    print('[R^2:', str(r_2) + ']')\n",
    "    \n",
    "    return alpha_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_kfold_cv(tree, param_dist, X_train, y_train, n_splits = 5, n_iter = 200, random_state=100):\n",
    "    \"\"\"\n",
    "    This function takes an *instantiated* Decision Tree/Random Forest, and performs cross validation on the input data.\n",
    "    \n",
    "    Input: tree,             a DecisionTreeRegressor/RandomForestRegressor (after instantiation)\n",
    "           param_dict,       a dictionary of parameter values you would like to test\n",
    "           X_train, y_train, output from preprocess_data()\n",
    "           n_splits,         number of folds you would like for your cv\n",
    "           n_iter,           number of samples of parameter settings (more = slower, but chance for better model)\n",
    "           random_state,     seed of the cross-validation for reproducibility\n",
    "           \n",
    "    Output: print statements of the optimal parameter values + training set MSE\n",
    "    \"\"\"\n",
    "\n",
    "    rnd_search = RandomizedSearchCV(tree, param_distributions=param_dist, \n",
    "                                    cv=n_splits, n_iter=n_iter, random_state = random_state)\n",
    "    rnd_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(rnd_search.best_score_)\n",
    "    print(rnd_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example param dists for decision trees and random forests\n",
    "\n",
    "Make sure to instantiate the `DecisionTreeRegressor` or `RandomForestRegressor` with the same variables as those you have in the `param_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "tree_param_dist = {'max_leaf_nodes': randint(3, 100),\n",
    "                  'max_features': randint(2, 25),\n",
    "                  'max_depth': randint(1, 10)}\n",
    "\n",
    "forest_param_dist = {'max_leaf_nodes': randint(3, 100),\n",
    "                  'max_features': randint(2, 25),\n",
    "                  'max_depth': randint(1, 10),\n",
    "                  'n_estimators': randint(50, 200)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection(model, X_train, y_train, n_splits=5, k_features=None):\n",
    "    \"\"\"\n",
    "    This function takes an *instantiated* Decision Tree/Random Forest, and chooses the best set of k features\n",
    "    to fit your data using forward selection.\n",
    "    \n",
    "    Input: model,            any sklearn model *after* instantiation\n",
    "           X_train, y_train, output from preprocess_data()\n",
    "           n_splits,         number of folds you would like for your cv\n",
    "           k_features,       number of features you would like your model to have\n",
    "           \n",
    "    Output: list of chosen columns of X_train\n",
    "    \"\"\"\n",
    "    \n",
    "    sfs = SFS(model, k_features = k_features, cv = n_splits, forward=True)\n",
    "    sfs.fit(X_train, y_train)\n",
    "    \n",
    "    return sfs.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of how to run the model on different number of features\n",
    "# m = LinearRegression()  \n",
    "# for i in range(1,6):  \n",
    "#     print(forward_selection(m, X_train, y_train, n_splits=5, k_features=i))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
