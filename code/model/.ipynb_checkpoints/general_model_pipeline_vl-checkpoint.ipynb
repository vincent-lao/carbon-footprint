{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Model Data Pipeline\n",
    "Created by Vincent Lao.\n",
    "\n",
    "### Pipeline:\n",
    "clean data $\\rightarrow$ `preprocess_data()` $\\rightarrow$ `run_model()`  \n",
    "$\\hspace{4cm} or$  \n",
    "clean data $\\rightarrow$ `preprocess_data()` $\\rightarrow$ `linreg_kfold_cv()`\n",
    "\n",
    "### Function of Pipeline:\n",
    "\n",
    "Given a dataset, y variable column name, and a defined model (e.g. LinearRegression()), do the following:  \n",
    "1. Split the data into training and testing data.\n",
    "2. Train the model on the training set, and predict on the test set.\n",
    "3. Calculate the MSE, and plot some diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, y_col, test_size=0.2, random_state=100, standardize = False):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and splits it up into training and testing data, \n",
    "    as well as features and response variable.\n",
    "    \n",
    "    Input: data,         a Pandas dataframe\n",
    "           y_col,        a string that is the name of the response variable\n",
    "           test_size,    a float between 0 and 1 indicating the fraction of the data to include in the test split\n",
    "           random_state, an integer, used to define the random state\n",
    "           standardize,    a boolean indicating if you would like to standardize your features.\n",
    "           \n",
    "    Output: X_train, 2D array of the training data feature matrix\n",
    "           X_test,  1D array of the training data response variable\n",
    "           y_train, 2D array of the testing data feature matrix\n",
    "           y_test,  1D array of the testing data response variable\n",
    "    \"\"\"\n",
    "    print('Splitting data...')\n",
    "    X = data.loc[:, data.columns != y_col]\n",
    "    y = data[y_col]\n",
    "    \n",
    "    if standardize:\n",
    "        scaler = StandardScaler() # Initialize the StandardScaler\n",
    "        scaler.fit(X)             # Fit the standard scaler for each feature\n",
    "        X = scaler.transform(X)   # Standardize each feature\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                                            test_size=test_size, \\\n",
    "                                                            random_state=random_state)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def run_model(X_train, X_test, y_train, y_test, model, diagnostics = False):\n",
    "    \"\"\"\n",
    "    This function takes in data that has been split by scikit-learn's test_train_split\n",
    "    and a model that has been initialized, and fits the model and calculates the MSE on the training & testing data.\n",
    "    \n",
    "    Input: X_train, X_test, y_train, y_test, the output of preprocess_data()\n",
    "           model, an initialized scikit-learn model, i.e. LinearRegression, Ridge(), Lasso(), DecisionTree()\n",
    "           diagnostics, a boolean indicating if you would like to plot linear regression diagnostic plots\n",
    "           \n",
    "    Ouput: printed R^2 and MSE values of both the training and testing sets\n",
    "           model, the fitted model\n",
    "    \"\"\"\n",
    "\n",
    "    # fit model with scikit-learn\n",
    "    print('Fitting Model...')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds_train = model.predict(X_train)\n",
    "    mse_train = mean_squared_error(preds_train, y_train.values)\n",
    "    print('[Train MSE:', str(np.round(mse_train, 4)) + ']')\n",
    "    \n",
    "    r_2 = model.score(X_test, y_test)\n",
    "    print('[Train R^2:', str(r_2) + ']')\n",
    "    \n",
    "    # evaluate model\n",
    "    preds_test = model.predict(X_test)\n",
    "    mse_test = mean_squared_error(preds_test, y_test.values)\n",
    "    print('[Test MSE:', str(np.round(mse_test, 4)) + ']')\n",
    "    \n",
    "    r_2 = model.score(X_test, y_test)\n",
    "    print('[Test R^2:', str(r_2) + ']')\n",
    "    \n",
    "    # plot any diagonostics that are in the \n",
    "    if diagnostics:\n",
    "        summarize_diagnostics(preds_test, y_test)\n",
    "\n",
    "    print('----FINISHED----')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def summarize_diagnostics(preds, y_test):\n",
    "    \n",
    "    df = pd.DataFrame({'preds' : preds, 'resids' : y_test - preds}).sort_values('preds')\n",
    "\n",
    "    # plot residuals\n",
    "    plt.title('Fitted Values vs. Residuals')\n",
    "    plt.scatter(df['preds'], df['resids'], color='blue')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.xlabel('Fitted Values')\n",
    "\n",
    "    # save plot to file\n",
    "#     plt.savefig('../../visualizations/diagnostic_plot.png')\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_kfold_cv(ModelCV, X_train, X_test, y_train, y_test, alphas, n_splits = 5, random_state=100):\n",
    "    \"\"\"\n",
    "    This function takes either RidgeCV or LassoCV, and performs cross validation on the input data.\n",
    "    Be careful to standardize the input data beforehand!\n",
    "    \n",
    "    Input: ModelCV,      either RidgeCV or LassoCV (before initializing it).\n",
    "           X_train,      X_test, y_train, y_test, output from preprocess_data()\n",
    "           alphas,       a list or array of alpha values you would like to test\n",
    "           n_splits,     number of folds you would like for your cv\n",
    "           random_state, seed of the cross-validation for reproducibility\n",
    "           \n",
    "    Output: print statements of the optimal alpha value + test set MSE\n",
    "            alpha_opt, the chosen optimal alpha value on the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    kf = KFold(n_splits = n_splits, shuffle=True, random_state=random_state)\n",
    "    cv = ModelCV(cv = kf, alphas = alphas)\n",
    "    cv.fit(X_train, y_train)\n",
    "    \n",
    "    alpha_opt = cv.alpha_\n",
    "    print(\"optimal alpha:\", alpha_opt)\n",
    "    \n",
    "    y_pred_cv = cv.predict(X_test)\n",
    "    \n",
    "    cv_mse = mean_squared_error(y_test, y_pred_cv)\n",
    "    print(\"Test MSE with cross-validated\", re.findall(r'\\w{5}CV', str(ModelCV))[0] + \":\", cv_mse)\n",
    "    \n",
    "    r_2 = cv.score(X_test, y_test)\n",
    "    print('[R^2:', str(r_2) + ']')\n",
    "    \n",
    "    return alpha_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreating the lm() function from R in Python using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from here: https://towardsdatascience.com/going-from-r-to-python-linear-regression-diagnostic-plots-144d1c4aa5a\n",
    "\n",
    "def plot_residuals_vs_fitted(results):\n",
    "    '''\n",
    "    Given a fitted statsmodels.OLS model, graph the residuals vs. fitted data. Code taken from link above.\n",
    "    '''\n",
    "    residuals = results.resid\n",
    "    fitted = results.fittedvalues\n",
    "    smoothed = lowess(residuals,fitted)\n",
    "    top3 = abs(residuals).sort_values(ascending = False)[:3]\n",
    "\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.rcParams[\"figure.figsize\"] = (8,7)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(fitted, residuals, edgecolors = 'k', facecolors = 'none')\n",
    "    ax.plot(smoothed[:,0],smoothed[:,1],color = 'r')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    ax.set_xlabel('Fitted Values')\n",
    "    ax.set_title('Residuals vs. Fitted')\n",
    "    ax.plot([min(fitted),max(fitted)],[0,0],color = 'k',linestyle = ':', alpha = .3)\n",
    "\n",
    "    for i in top3.index:\n",
    "        ax.annotate(i,xy=(fitted[i],residuals[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qqplot(results):\n",
    "    '''\n",
    "    Given a fitted statsmodels.OLS model, graph the normal Q-Q plot. Code taken from link above.\n",
    "    '''\n",
    "    sorted_student_residuals = pd.Series(results.get_influence().resid_studentized_internal)\n",
    "    sorted_student_residuals.index = results.resid.index\n",
    "    sorted_student_residuals = sorted_student_residuals.sort_values(ascending = True)\n",
    "    df = pd.DataFrame(sorted_student_residuals)\n",
    "    df.columns = ['sorted_student_residuals']\n",
    "    df['theoretical_quantiles'] = stats.probplot(df['sorted_student_residuals'], dist = 'norm', fit = False)[0]\n",
    "    rankings = abs(df['sorted_student_residuals']).sort_values(ascending = False)\n",
    "    top3 = rankings[:3]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    x = df['theoretical_quantiles']\n",
    "    y = df['sorted_student_residuals']\n",
    "    ax.scatter(x,y, edgecolor = 'k',facecolor = 'none')\n",
    "    ax.set_title('Normal Q-Q')\n",
    "    ax.set_ylabel('Standardized Residuals')\n",
    "    ax.set_xlabel('Theoretical Quantiles')\n",
    "    ax.plot([np.min([x,y]),np.max([x,y])],[np.min([x,y]),np.max([x,y])], color = 'r', ls = '--')\n",
    "    for val in top3.index:\n",
    "        ax.annotate(val,xy=(df['theoretical_quantiles'].loc[val],df['sorted_student_residuals'].loc[val]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_leverage(results):\n",
    "    '''\n",
    "    Given a fitted statsmodels.OLS model, graph the residuals vs. leverage plot. Code taken from link above.\n",
    "    '''\n",
    "    student_residuals = pd.Series(results.get_influence().resid_studentized_internal)\n",
    "    student_residuals.index = results.resid.index\n",
    "    df = pd.DataFrame(student_residuals)\n",
    "    df.columns = ['student_residuals']\n",
    "    df['leverage'] = results.get_influence().hat_matrix_diag\n",
    "    smoothed = lowess(df['student_residuals'],df['leverage'])\n",
    "    sorted_student_residuals = abs(df['student_residuals']).sort_values(ascending = False)\n",
    "    top3 = sorted_student_residuals[:3]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    x = df['leverage']\n",
    "    y = df['student_residuals']\n",
    "    xpos = max(x)+max(x)*0.01  \n",
    "    ax.scatter(x, y, edgecolors = 'k', facecolors = 'none')\n",
    "    ax.plot(smoothed[:,0],smoothed[:,1],color = 'r')\n",
    "    ax.set_ylabel('Studentized Residuals')\n",
    "    ax.set_xlabel('Leverage')\n",
    "    ax.set_title('Residuals vs. Leverage')\n",
    "    ax.set_ylim(min(y)-min(y)*0.15,max(y)+max(y)*0.15)\n",
    "    ax.set_xlim(-0.01,max(x)+max(x)*0.05)\n",
    "    plt.tight_layout()\n",
    "    for val in top3.index:\n",
    "        ax.annotate(val,xy=(x.loc[val],y.loc[val]))\n",
    "\n",
    "    cooksx = np.linspace(min(x), xpos, 50)\n",
    "    p = len(results.params)\n",
    "    poscooks1y = np.sqrt((p*(1-cooksx))/cooksx)\n",
    "    poscooks05y = np.sqrt(0.5*(p*(1-cooksx))/cooksx)\n",
    "    negcooks1y = -np.sqrt((p*(1-cooksx))/cooksx)\n",
    "    negcooks05y = -np.sqrt(0.5*(p*(1-cooksx))/cooksx)\n",
    "\n",
    "    ax.plot(cooksx,poscooks1y,label = \"Cook's Distance\", ls = ':', color = 'r')\n",
    "    ax.plot(cooksx,poscooks05y, ls = ':', color = 'r')\n",
    "    ax.plot(cooksx,negcooks1y, ls = ':', color = 'r')\n",
    "    ax.plot(cooksx,negcooks05y, ls = ':', color = 'r')\n",
    "    ax.plot([0,0],ax.get_ylim(), ls=\":\", alpha = .3, color = 'k')\n",
    "    ax.plot(ax.get_xlim(), [0,0], ls=\":\", alpha = .3, color = 'k')\n",
    "    ax.annotate('1.0', xy = (xpos, poscooks1y[-1]), color = 'r')\n",
    "    ax.annotate('0.5', xy = (xpos, poscooks05y[-1]), color = 'r')\n",
    "    ax.annotate('1.0', xy = (xpos, negcooks1y[-1]), color = 'r')\n",
    "    ax.annotate('0.5', xy = (xpos, negcooks05y[-1]), color = 'r')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using statsmodels \n",
    "\n",
    "# X = sm.add_constant(food2.loc[:, food2.columns != 'totalco2']).drop(columns='soybeansupply_pct')\n",
    "# y = food2['totalco2']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "#                                                         test_size=0.2, \\\n",
    "#                                                         random_state=100)\n",
    "\n",
    "# model = sm.OLS(y_train, X_train)\n",
    "# results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
